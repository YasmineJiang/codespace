{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+uBdooIb8me+Kc39OALY4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasmineJiang/codespace/blob/main/Web_Analytics_Model_Deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iWcN87GePJrW"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "class UserPredictor():\n",
        "    def __init__(self):\n",
        "\n",
        "        self.model = Pipeline([(\"pf\", PolynomialFeatures(degree=2, include_bias=False)),(\"lr\", LogisticRegression())\n",
        "])\n",
        "\n",
        "\n",
        "    def data_manipulation(self,users,logs):\n",
        "\n",
        "        #Feature columnsâ€˜ data preparation:\n",
        "        #Turn the categorical columns in \"train_users\" to numerical data\n",
        "        feature_1 = users[['user_id','age','past_purchase_amt','badge']]\n",
        "        oh = OneHotEncoder()\n",
        "        badge_df = pd.DataFrame(oh.fit_transform(feature_1[['badge']]).toarray(), columns=oh.get_feature_names_out())\n",
        "        feature_user = pd.concat([feature_1[[\"user_id\",\"age\",\"past_purchase_amt\"]], badge_df],axis=1)\n",
        "\n",
        "        #Genrate new feature 'avg' in \"train_logs\" dataset\n",
        "        #Computing 'avg' for user_id that have visited /laptop url at least once\n",
        "        logs.reset_index(drop=True)\n",
        "        df_grouped = pd.DataFrame()\n",
        "        df_grouped['avg'] = pd.DataFrame(logs[logs['url'] == '/laptop.html'].groupby(['user_id','url'])['seconds'].mean())\n",
        "        df_grouped.reset_index()\n",
        "\n",
        "        #Replace NAN with 0 to make sure no missing data in our dataset\n",
        "        logs['url'] = '/laptop.html'\n",
        "        df_avg = pd.merge(logs,df_grouped,how='left',on=['user_id','url'])\n",
        "        values = {'avg':0}\n",
        "        df_avg.fillna(value=values,inplace=True)\n",
        "        feature_logs = df_avg[['user_id','url','avg']].drop_duplicates()\n",
        "\n",
        "        #Merge \"feature_logs\" with \"feature_users\" and replace NAN with 0\n",
        "        all_feature = pd.merge(feature_user,feature_logs[['user_id','avg']],how='left',on='user_id')\n",
        "        all_feature.fillna(value=values,inplace=True)\n",
        "        all_feature.set_index('user_id',inplace=True)\n",
        "\n",
        "        #Standardize the feature dataset\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(all_feature)\n",
        "        all_feature_trans = pd.DataFrame(scaler.transform(all_feature),columns=list(all_feature.columns))\n",
        "\n",
        "        return all_feature_trans\n",
        "\n",
        "\n",
        "    def fit(self,train_users,train_logs,train_y):\n",
        "        users = train_users\n",
        "        logs = train_logs\n",
        "\n",
        "        train_final_df = pd.concat([self.data_manipulation(users,logs),train_y],axis=1)\n",
        "        #Split dataset into train & test\n",
        "        xcols = ['age','past_purchase_amt','badge_bronze','badge_gold','badge_silver','avg']\n",
        "        ycol = 'y'\n",
        "        train, test = train_test_split(train_final_df)\n",
        "\n",
        "        self.model.fit(train[xcols], train[ycol])\n",
        "        self.model.predict((test[xcols]))\n",
        "\n",
        "    def predict(self,test_users,test_logs):\n",
        "\n",
        "        users = test_users\n",
        "        logs = test_logs\n",
        "\n",
        "        #Split dataset into train & test\n",
        "        xcols = ['age','past_purchase_amt','badge_bronze','badge_gold','badge_silver','avg']\n",
        "        ycol = 'y'\n",
        "        return self.model.predict((self.data_manipulation(users,logs)))\n",
        "\n",
        "        #return self.model.predict((test_final_df[xcols]))"
      ]
    }
  ]
}